{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "1. For each type of model:\n",
    "    - For each of 5 folds, predict validation set and full test set (OOF: out of fold).\n",
    "    - Assemble one column with the 5 validation predictions --> to `train_df`\n",
    "    - Assemble one column with the mean of test predictions --> to `test_df`\n",
    "2. Concatenate the data from `train_df` and `test_df` with the statistical properties.\n",
    "3. Do CV as we were doing before on the new data. **Note:** the csv files in the \"Final CV\" section are three different submissions. Each of them is a \"normal\" CV.\n",
    "\n",
    "*****\n",
    "\n",
    "The files `train_df2.csv` and `test_df2.csv` in the `stacking` directory also include the geometric mean in the statistical properties (see the definition of `stats_prop`). Didn't result in any difference in local CV, but maybe it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "\n",
    "# basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "sns.set(palette='colorblind')\n",
    "\n",
    "# processing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import stats\n",
    "\n",
    "import shap\n",
    "\n",
    "# models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_prop(df):\n",
    "    tr_means = []\n",
    "    tr_medians = []\n",
    "    tr_stds = []\n",
    "    tr_skews = []\n",
    "    tr_mins = []\n",
    "    tr_maxs = []\n",
    "    tr_sums = []\n",
    "    tr_nonzero = []\n",
    "    tr_prods = []\n",
    "    for i, row in df.iterrows():\n",
    "        tr_means.append(row[row.nonzero()[0]].mean())\n",
    "        tr_medians.append(row[row.nonzero()[0]].median())\n",
    "        tr_stds.append(row[row.nonzero()[0]].std())\n",
    "        tr_skews.append(row[row.nonzero()[0]].skew())\n",
    "        tr_mins.append(row[row.nonzero()[0]].min())\n",
    "        tr_maxs.append(row[row.nonzero()[0]].max())\n",
    "        tr_sums.append(row[row.nonzero()[0]].sum())\n",
    "        tr_nonzero.append(row[row.nonzero()[0]].count()/df.shape[1])\n",
    "        tr_prods.append(stats.gmean(row[row.nonzero()[0]]))\n",
    "        \n",
    "    tr_means = np.nan_to_num(np.array(tr_means))\n",
    "    tr_medians = np.nan_to_num(np.array(tr_medians))\n",
    "    tr_stds = np.nan_to_num(np.array(tr_stds))\n",
    "    tr_skews = np.nan_to_num(np.array(tr_skews))\n",
    "    tr_mins = np.nan_to_num(np.array(tr_mins))\n",
    "    tr_maxs = np.nan_to_num(np.array(tr_maxs))\n",
    "    tr_sums = np.nan_to_num(np.array(tr_sums))\n",
    "    tr_nonzero = np.nan_to_num(np.array(tr_nonzero))\n",
    "    tr_prods = np.nan_to_num(np.array(tr_prods))\n",
    "        \n",
    "    return np.stack((tr_means, tr_medians, tr_stds, tr_skews, tr_mins, tr_maxs, tr_sums, tr_nonzero, tr_prods), axis=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### divide by 1000 to overestimate later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data.columns[1:]] = train_data[train_data.columns[1:]]/1000\n",
    "test_data[test_data.columns[1:]] = test_data[test_data.columns[1:]]/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "#### Removing target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove target and ID columns\n",
    "target = train_data.target\n",
    "train_ID = train_data.ID\n",
    "train_data = train_data.drop(['target', 'ID'], axis=1)\n",
    "test_ID = test_data.ID\n",
    "test_data = test_data.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_var = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = VarianceThreshold(threshold=threshold_var)\n",
    "vt.fit(train_data)\n",
    "selected_columns = train_data.columns[vt.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_novar = train_data[selected_columns]\n",
    "test_novar = test_data[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Remaining columns: %i' % len(selected_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_features_train = train_novar\n",
    "log_features_test = test_novar\n",
    "log_target = np.log1p(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stat all cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_train = stats_prop(log_features_train)\n",
    "stat_test = stats_prop(log_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV folds\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "n_splits = kf.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "params = {'learning_rate':0.5, 'metric':'rmse', 'max_bin':63, 'device':'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = np.zeros((log_features_train.shape[0],log_features_train.shape[1]+1))\n",
    "\n",
    "for train_index, test_index in kf.split(log_features_train):\n",
    "    \n",
    "    X_tr, X_tst = log_features_train.values[train_index], log_features_train.values[test_index]\n",
    "    y_tr, y_tst = log_target[train_index], log_target[test_index]\n",
    "    \n",
    "    train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "    valid_set = lgb.Dataset(X_tst, label=y_tst, reference=train_set)\n",
    "    \n",
    "    bst = lgb.train(params, train_set, num_boost_round=50, valid_sets=[train_set, valid_set], early_stopping_rounds=5,\n",
    "                    verbose_eval=False)\n",
    "\n",
    "    shap_values += shap.TreeExplainer(bst).shap_values(log_features_train.values)/n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, log_features_train, max_display=10, plot_type='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 150\n",
    "sorted_columns = np.argsort(np.sum(np.abs(shap_values), axis=0)[:-1])[::-1]\n",
    "most_relevant = train_novar.columns[sorted_columns[:N]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_features_train = log_features_train[most_relevant]\n",
    "log_features_test = log_features_test[most_relevant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Remaining columns: %i' % log_features_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = np.hstack((log_features_train.values, stat_train))\n",
    "test_final = np.hstack((log_features_test.values, stat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data, test_data, train_novar, test_novar, log_features_train, log_features_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final number of features: %i' % train_final.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oof train+test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "n_splits = kf.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame.from_dict({'ID': train_ID, 'xgb': np.zeros(train_ID.shape[0]), 'lgb': np.zeros(train_ID.shape[0]),\n",
    "                                  'cb': np.zeros(train_ID.shape[0])})\n",
    "test_df = pd.DataFrame.from_dict({'ID': test_ID, 'xgb': np.zeros(test_ID.shape[0]), 'lgb': np.zeros(test_ID.shape[0]),\n",
    "                                 'cb': np.zeros(test_ID.shape[0])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "\n",
    "params = {'booster': 'gbtree', 'learning_rate':0.003, 'colsample_bytree': 0.05, 'eval_metric':'rmse', 'lambda': 3.,\n",
    "          'alpha': 0.03}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "test_set = xgb.DMatrix(test_final)\n",
    "\n",
    "for train_index, test_index in kf.split(train_final):\n",
    "        \n",
    "    X_tr, X_tst = train_final[train_index], train_final[test_index]\n",
    "    y_tr, y_tst = log_target[train_index], log_target[test_index]\n",
    "\n",
    "    train_set = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    valid_set = xgb.DMatrix(X_tst, label=y_tst)\n",
    "\n",
    "    bst = xgb.train(params, train_set, num_boost_round=20000, evals=[(train_set, 'train'), (valid_set, 'val')],\n",
    "                    early_stopping_rounds=500, verbose_eval=1000)\n",
    "\n",
    "    y_val = bst.predict(valid_set, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "    rmsle = np.sqrt(mean_squared_error(y_tst, y_val))\n",
    "\n",
    "    print('RMSLE: %.5f' % rmsle)\n",
    "\n",
    "    errors.append(rmsle)\n",
    "    \n",
    "    train_df.xgb[test_index] = np.expm1(y_val)\n",
    "\n",
    "    test_df.xgb = test_df.xgb + np.expm1(bst.predict(test_set, ntree_limit=bst.best_ntree_limit))/n_splits\n",
    "        \n",
    "print('\\n Fold mean of RMSLE: %.5f' % np.mean(errors))\n",
    "print('\\n Fold std of RMSLE: %.5f' % np.std(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "params = {'boosting': 'gbdt', 'objective':'regression', 'learning_rate':0.01, 'metric':'rmse', 'max_bin':63, \n",
    "          'lambda_l2': 0.05, 'device':'cpu', 'feature_fraction': 0.075, 'lambda_l1': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "for train_index, test_index in kf.split(train_final):\n",
    "        \n",
    "    X_tr, X_tst = train_final[train_index], train_final[test_index]\n",
    "    y_tr, y_tst = log_target[train_index], log_target[test_index]\n",
    "\n",
    "    train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "    valid_set = lgb.Dataset(X_tst, label=y_tst, reference=train_set)\n",
    "\n",
    "    bst = lgb.train(params, train_set, num_boost_round=20000, valid_sets=[train_set, valid_set], early_stopping_rounds=500, \n",
    "                    verbose_eval=1000)\n",
    "\n",
    "    y_val = bst.predict(X_tst, num_iteration=bst.best_iteration)\n",
    "\n",
    "    rmsle = np.sqrt(mean_squared_error(y_tst, y_val))\n",
    "\n",
    "    print('RMSLE: %.5f' % rmsle)\n",
    "\n",
    "    errors.append(rmsle)\n",
    "    \n",
    "    train_df.lgb[test_index] = np.expm1(y_val)\n",
    "    \n",
    "    test_df.lgb = test_df.lgb + np.expm1(bst.predict(test_final, num_iteration=bst.best_iteration))/n_splits\n",
    "        \n",
    "print('\\n Fold mean of RMSLE: %.5f' % np.mean(errors))\n",
    "print('\\n Fold std of RMSLE: %.5f' % np.std(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "for train_index, test_index in kf.split(train_final):\n",
    "        \n",
    "    X_tr, X_tst = train_final[train_index], train_final[test_index]\n",
    "    y_tr, y_tst = log_target[train_index], log_target[test_index]\n",
    "\n",
    "    bst = cb.CatBoostRegressor(eta=0.01,iterations=5000, loss_function='RMSE', eval_metric='RMSE', depth=5)\n",
    "    \n",
    "    bst.fit(X_tr, y_tr, use_best_model=True, eval_set=(X_tst, y_tst), verbose=True)\n",
    "\n",
    "    y_val = bst.predict(X_tst)\n",
    "\n",
    "    rmsle = np.sqrt(mean_squared_error(y_tst, y_val))\n",
    "\n",
    "    print('RMSLE: %.5f' % rmsle)\n",
    "\n",
    "    errors.append(rmsle)\n",
    "    \n",
    "    train_df.cb[test_index] = np.expm1(y_val)\n",
    "    \n",
    "    test_df.cb = test_df.cb + np.expm1(bst.predict(test_final))/n_splits\n",
    "        \n",
    "print('\\n Fold mean of RMSLE: %.5f' % np.mean(errors))\n",
    "print('\\n Fold std of RMSLE: %.5f' % np.std(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../submission_files/stacking/train_df_2.csv', index=False)\n",
    "test_df.to_csv('../submission_files/stacking/test_df_2.csv', index=False)\n",
    "#train_df = pd.read_csv('../submission_files/stacking/train_df.csv')\n",
    "#test_df = pd.read_csv('../submission_files/stacking/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['ID'], axis=1)\n",
    "test_df = test_df.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.values\n",
    "test_df = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = np.hstack((train_df, stat_train))\n",
    "test_df2 = np.hstack((test_df, stat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final CV\n",
    "\n",
    "#### cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({'ID': test_ID, 'target': np.zeros(test_ID.shape[0])})\n",
    "\n",
    "errors = []\n",
    "\n",
    "for train_index, test_index in kf.split(train_df2):\n",
    "        \n",
    "    X_tr, X_tst = train_df2[train_index], train_df2[test_index]\n",
    "    y_tr, y_tst = log_target[train_index], log_target[test_index]\n",
    "\n",
    "    bst = cb.CatBoostRegressor(eta=0.1,iterations=200, loss_function='RMSE', eval_metric='RMSE', depth=4)\n",
    "    \n",
    "    bst.fit(X_tr, y_tr, use_best_model=True, eval_set=(X_tst, y_tst), verbose=True)\n",
    "\n",
    "    y_val = bst.predict(X_tst)\n",
    "\n",
    "    rmsle = np.sqrt(mean_squared_error(y_tst, y_val))\n",
    "\n",
    "    print('RMSLE: %.5f' % rmsle)\n",
    "\n",
    "    errors.append(rmsle)\n",
    "    \n",
    "    submission.target = submission.target + np.expm1(bst.predict(test_df2))/n_splits\n",
    "        \n",
    "print('\\n Fold mean of RMSLE: %.5f' % np.mean(errors))\n",
    "print('\\n Fold std of RMSLE: %.5f' % np.std(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ceiling to overestimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.target = 1000*np.ceil(submission.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../submission_files/stacking/cb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "params = {'boosting': 'gbdt', 'objective':'regression', 'learning_rate':0.01, 'metric':'rmse', 'max_bin':63, \n",
    "          'lambda_l2': 0.05, 'device':'cpu', 'feature_fraction': .44, 'lambda_l1': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({'ID': test_ID, 'target': np.zeros(test_ID.shape[0])})\n",
    "\n",
    "errors = []\n",
    "\n",
    "for train_index, test_index in kf.split(train_df2):\n",
    "        \n",
    "    X_tr, X_tst = train_df2[train_index], train_df2[test_index]\n",
    "    y_tr, y_tst = log_target[train_index], log_target[test_index]\n",
    "\n",
    "    train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "    valid_set = lgb.Dataset(X_tst, label=y_tst, reference=train_set)\n",
    "\n",
    "    bst = lgb.train(params, train_set, num_boost_round=20000, valid_sets=[train_set, valid_set], early_stopping_rounds=500, \n",
    "                    verbose_eval=1000)\n",
    "\n",
    "    y_val = bst.predict(X_tst, num_iteration=bst.best_iteration)\n",
    "    \n",
    "    rmsle = np.sqrt(mean_squared_error(y_tst, y_val))\n",
    "\n",
    "    print('RMSLE: %.5f' % rmsle)\n",
    "\n",
    "    errors.append(rmsle)\n",
    "    \n",
    "    submission.target = submission.target + np.expm1(bst.predict(test_df2, num_iteration=bst.best_iteration))/n_splits\n",
    "        \n",
    "print('\\n Fold mean of RMSLE: %.5f' % np.mean(errors))\n",
    "print('\\n Fold std of RMSLE: %.5f' % np.std(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ceiling to overestimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.target = 1000*np.ceil(submission.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../submission_files/stacking/lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "params = {'booster': 'gbtree', 'learning_rate':0.01, 'colsample_bytree': 0.5, 'eval_metric':'rmse', 'lambda': 3.,\n",
    "          'alpha': 0.03}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({'ID': test_ID, 'target': np.zeros(test_ID.shape[0])})\n",
    "\n",
    "errors = []\n",
    "\n",
    "test_set = xgb.DMatrix(test_df2)\n",
    "\n",
    "for train_index, test_index in kf.split(train_df2):\n",
    "        \n",
    "    X_tr, X_tst = train_df2[train_index], train_df2[test_index]\n",
    "    y_tr, y_tst = log_target[train_index], log_target[test_index]\n",
    "\n",
    "    train_set = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    valid_set = xgb.DMatrix(X_tst, label=y_tst)\n",
    "\n",
    "    bst = xgb.train(params, train_set, num_boost_round=5000, evals=[(train_set, 'train'), (valid_set, 'val')],\n",
    "                    early_stopping_rounds=500, verbose_eval=1000)\n",
    "\n",
    "    y_val = bst.predict(valid_set, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "    rmsle = np.sqrt(mean_squared_error(y_tst, y_val))\n",
    "\n",
    "    print('RMSLE: %.5f' % rmsle)\n",
    "\n",
    "    errors.append(rmsle)\n",
    "    \n",
    "    submission.target = submission.target + np.expm1(bst.predict(test_set, ntree_limit=bst.best_ntree_limit))/n_splits\n",
    "        \n",
    "print('\\n Fold mean of RMSLE: %.5f' % np.mean(errors))\n",
    "print('\\n Fold std of RMSLE: %.5f' % np.std(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ceiling to overestimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.target = 1000*np.ceil(submission.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../submission_files/stacking/xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
